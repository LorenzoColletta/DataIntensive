{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRlZanLmPzG8"
   },
   "source": [
    "Lorenzo Colletta, Giorgio Fantilli, Luca Lucioli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbIORchGW1oz"
   },
   "source": [
    "## Parte 1 - Descrizione del problema\n",
    "\n",
    "Il dataset in esame include dati per la stima dei livelli di obesità negli individui provenienti dai paesi di Messico, Perù e Colombia, in base alle loro abitudini alimentari e alle condizioni fisiche.\n",
    "Le istanze sono classificate in base al livello di obesità (Peso Insufficiente, Peso Normale, Sovrappeso Livello I, Sovrappeso Livello II, Obesità Tipo I, Obesità Tipo II e Obesità Tipo III). Il 77% dei dati è stato generato artificialmente utilizzando lo strumento Weka e il filtro SMOTE, mentre il 23% dei dati è stato raccolto direttamente dagli utenti attraverso una piattaforma web.\n",
    "\n",
    "L'obiettivo è predire il livello di obesità dei soggetti utilizzando i loro dati antropometrici, le informazioni sulle loro abitudini alimentari e sull'attività fisica.\n",
    "\n",
    "Di seguito vengono importate le librerie necessarie per scaricare i file, organizzare le strutture dati e disegnare i grafici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECDeFnTwQybP"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dython.nominal import associations, cramers_v\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGtjGX1HVOyH"
   },
   "source": [
    "### Caricamento dati e preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O5CGPhUQe5b"
   },
   "outputs": [],
   "source": [
    "file_zip_url = \"https://archive.ics.uci.edu/static/public/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition.zip\"\n",
    "file_zip_name = \"obesity_data.zip\"\n",
    "file = \"./ObesityDataSet_raw_and_data_sinthetic.csv\"\n",
    "\n",
    "if not os.path.exists(file_zip_name):\n",
    "    from urllib.request import urlretrieve\n",
    "    urlretrieve(file_zip_url, file_zip_name)\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(file_zip_name) as f:\n",
    "        f.extractall()\n",
    "\n",
    "obesity_raw_data = pd.read_csv(file)\n",
    "obesity_raw_data = obesity_raw_data.rename(columns={\"family_history_with_overweight\" : \"FHWO\", \"NObeyesdad\" : \"ObesityLevel\"})\n",
    "\n",
    "obesity_raw_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCJQeAsEuTAU"
   },
   "source": [
    "Sopra viene mostrato un piccolo estratto del dataset in esame, dopo aver rinominato alcune feature per migliorarne la leggibilità e l'utilizzo.\n",
    "\n",
    "Di seguito sono riportate le dimensioni in memoria, il numero di istanze non nulle e il tipo delle feature che compongono i dati raccolti nel dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63JGnCPM6FN1"
   },
   "outputs": [],
   "source": [
    "obesity_raw_data.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJ_wJWmb6RLz"
   },
   "source": [
    "Osserviamo come molte delle feature sono di tipo `object`. Procediamo di seguito con la trasformazione in variabili categoriche e booleane per limitare la memoria occupata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oE1VqA6ZXDoh"
   },
   "outputs": [],
   "source": [
    "categorical = [\"Gender\", \"CAEC\", \"CALC\", \"MTRANS\", \"ObesityLevel\"]\n",
    "boolean = [\"FHWO\", \"FAVC\", \"SMOKE\", \"SCC\"]\n",
    "\n",
    "obesity_raw_data[categorical] = obesity_raw_data[categorical].astype(\"category\")\n",
    "\n",
    "obesity_raw_data[boolean] = obesity_raw_data[boolean].replace({\"yes\": True, \"no\": False})\n",
    "\n",
    "obesity_raw_data.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhEWkZXb0Z_g"
   },
   "source": [
    "Osserviamo come tutte le feature sono rileventi per il problema, e soprattutto che nessuna delle stesse presenta valori nulli. Non risulta perciò necessaria nessuna pulizia dei dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hJ_Ee9NYhJl"
   },
   "source": [
    "### Descrizione delle feature\n",
    "Il dataset contiene le seguenti feature:\n",
    "*   `Gender`: genere (Male, Female)\n",
    "*   `Age`: età\n",
    "*   `Height`: altezza in metri\n",
    "*   `Weight`: peso in kilogrammi\n",
    "*   `FHWO`: indica se il soggetto ha familiari che sono o sono stati sovrappeso (yes, no)\n",
    "*   `FAVC`: indica se il soggetto mangia spesso cibo ad alto contenuto calorico o meno (yes, no)\n",
    "*   `FCVC`: indica se solitamente il soggetto mangia verdura durante i pasti o meno (Never, Sometimes, Always)\n",
    "*   `NCP`: indica quanti pasti consuma il soggetto durante la giornata (tra 1 e 2, 3, più di 3)\n",
    "*   `CAEC`: indica se il soggetto mangia tra i pasti (No, Sometimes, Frequently, Always)\n",
    "*   `SMOKE`: indica se il soggetto fuma o meno (yes, no)\n",
    "*   `CH2O`: indica quanta acqua beve il soggetto durante il giorno (meno di 1l, tra 1l e 2l, più di 2l)\n",
    "*   `SCC`: indica se il soggetto monitora le calorie che assume durante il giorno (yes, no)\n",
    "*   `FAF`: indica quanto spesso il soggetto svolge attività fisica durante la settimana (mai, 1 o 2 giorni, 2 o 4 giorni, 4 o 5 giorni)\n",
    "*   `TUE`: indica quanto spesso il soggetto utilizza apparecchi tecnologici durante il giorno come telefono, videogiochi, tv, computer o altri (0-2 ore, 3-5 ore, più di 5 ore)\n",
    "*   `CALC`: indica quanto spesso il soggetto beve alcool (Never, Sometimes, Frequently, Always)\n",
    "*   `MTRANS`: indica quale mezzo di trasporto utilizza solitamente il soggetto (Automobile, Motorbike, Bike, Public Transportation, Walking)\n",
    "\n",
    "La variabile target è `ObesityLevel`, che rappresenta il livello di obesità nelle seguenti classi: Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II, and Obesity Type III.\n",
    "\n",
    "Le features in esame sono sia categoriche, binarie, che continue:<br>\n",
    "Tra le categoriche troviamo: `Gender`, `CAEC`, `CALC` e `MTRANS`.<br>\n",
    "Tra le continue troviamo: `Age`, `Height`, `Weight`, `FCVC`, `NCP`, `CH2O`, `FAF` e `TUE`.<br>\n",
    "Tra le binarie troviamo: `FHWO`, `FAVC`, `SMOKE` e `SCC`.\n",
    "\n",
    "Si segnala che `FCVC`, `NCP`, `CH2O`, `FAF` e `TUE` sono feature di natura categorica ma assumono valori continui in quanto una parte dei dati è stata generata artificialmente. Si posticipa la scelta dell'approccio da utilizzare per la gestione di tali variabili."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gender nominale\n",
    "age continua \n",
    "height continua\n",
    "weight continua\n",
    "FHWO boolean\n",
    "FAVC boolean\n",
    "FCVC ordinale\n",
    "NCP ordinale\n",
    "CAEC ordinale\n",
    "SMOKE boolean\n",
    "CH2O ordinale\n",
    "SCC boolean\n",
    "FAF ordinale\n",
    "TUE ordinale\n",
    "CALC ordinale\n",
    "MTRANS nominale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVS4ZnU8kLDW"
   },
   "outputs": [],
   "source": [
    "obesity_raw_data[\"ObesityLevel\"].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "plt.ylabel(None)\n",
    "plt.figure().set_facecolor('white')\n",
    "plt.show()\n",
    "\n",
    "obesity_raw_data[\"ObesityLevel\"].value_counts().plot.bar(color='skyblue', rot=90, xlabel='Livello di Obesità', ylabel='Numero di Istanze', title='Distribuzione delle Istanze per Livello di Obesità')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v34ffOYCuaNn"
   },
   "source": [
    "Osserviamo dai grafici come la variabile target `ObesityLevel` risulta essere bilanciata, ovvero il numero di istanze per le possibili classi sono pressochè identiche.  Il problema in esame risulta quindi essere bilanciato e non e' necessario utilizzare tecniche di bilanciamento delle classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si verifica come nessuna delle feature del dataset presenta valori nulli. Osserviamo inoltre che tutte le variabili sono rileventi per la modellazione del problema. Non risulta perciò necessaria nessuna pulizia dei dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 - Analisi Esplorativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esplorazione delle feature continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito sono riportate le statistiche generali delle feature continue del dataset (media, minimo, massimo, deviazione standard e percentili) e vengono mostrati gli istogrammi relativi alla distribuzione dei loro valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P66wNiky5YRm"
   },
   "outputs": [],
   "source": [
    "obesity_raw_data[['Age', 'Height', 'Weight']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nota che i soggetti del campione hanno tra i 14 e i 61 anni. Rilevando che il 75% ha meno di 26 anni, la media è di circa 24 anni e la deviazione standard è 6.35, si può ipotizzare che la maggior parte dei soggetti ha tra i 20 e i 30 anni.<br>\n",
    "Il peso massimo registrato è pari a 173kg ma il 75% dei soggetti pesa meno di 107kg, è perciò probabile la presenza di valori di peso particolari nell'estremo superiore. Nell'estremo inferiore i valori sono invece coerenti con le caratteristiche antropometriche del campione (età e altezza)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Age')\n",
    "plt.hist(obesity_raw_data['Age'], label=\"Age\", color='gray')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('anni')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Weight')\n",
    "plt.hist(obesity_raw_data['Weight'], label=\"weight\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('Kg')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Height')\n",
    "plt.hist(obesity_raw_data['Height'], label=\"height\", color=\"tab:red\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L'istogramma dell'età conferma l'ipotesi di cui sopra: la maggior parte dei soggetti ha un età compresa tra i 20 e i 30 anni. Si può osservare come il numero delle istanze tenda a diminuire all'avanzare dell'età, diventando trascurabile nella parte finale del grafico.\n",
    "* Come precedentemente osservato, il grafico del peso rileva una distribuzione delle istanze congrua con il campione, ad eccezione dei valori più alti che risultano poco popolati.\n",
    "* Il grafico dell'altezza è conforme alla distribuzione attesa per la variabile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rimozione degli outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "plt.subplot(4, 3, 1)\n",
    "plt.title('Age')\n",
    "plt.boxplot(obesity_raw_data['Age'])\n",
    "plt.ylabel('anni')\n",
    "\n",
    "plt.subplot(4, 3, 2)\n",
    "plt.title('Weight')\n",
    "plt.boxplot(obesity_raw_data['Weight'])\n",
    "plt.ylabel('Kg')\n",
    "\n",
    "plt.subplot(4, 3, 3)\n",
    "plt.title('Height')\n",
    "plt.boxplot(obesity_raw_data['Height'])\n",
    "plt.ylabel('m')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I grafici sopra permettono di visualizzare meglio la distribuzione dei dati più estremi del campione.<br>\n",
    "Il primo box plot conferma la tendenza dei record a diminuire con l'aumentare dell'età, in particolare il numero di istanze che assumono un'età superiore a circa 50 anni diventa trascurabile.<br>\n",
    "Il secondo grafico mostra la presenza di istanze che assumono valori massimi anomali nell'estremità superiore per la variabile del peso, come precedentemente ipotizzato.<br>\n",
    "La distribuzione delle altezze si conferma corretta, a scanso di isolati valori limite superiori.\n",
    "\n",
    "Si procede di seguito, in base alle precedenti osservazioni, con la rimozione degli outliers sulla base delle deviazioni standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_raw_data.drop(obesity_raw_data.loc[abs(obesity_raw_data['Age'] - obesity_raw_data['Age'].mean()) > 3 * obesity_raw_data['Age'].std()].index, inplace=True)\n",
    "obesity_raw_data.drop(obesity_raw_data.loc[abs(obesity_raw_data['Weight'] - obesity_raw_data['Weight'].mean()) > 3 * obesity_raw_data['Weight'].std()].index, inplace=True)\n",
    "obesity_raw_data.drop(obesity_raw_data.loc[abs(obesity_raw_data['Height'] - obesity_raw_data['Height'].mean()) > 3 * obesity_raw_data['Height'].std()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 30))\n",
    "\n",
    "plt.subplot(4, 3, 1)\n",
    "plt.title('Age')\n",
    "plt.boxplot(obesity_raw_data['Age'])\n",
    "plt.ylabel('anni')\n",
    "\n",
    "plt.subplot(4, 3, 2)\n",
    "plt.title('Weight')\n",
    "plt.boxplot(obesity_raw_data['Weight'])\n",
    "plt.ylabel('Kg')\n",
    "\n",
    "plt.subplot(4, 3, 3)\n",
    "plt.title('Height')\n",
    "plt.boxplot(obesity_raw_data['Height'])\n",
    "plt.ylabel('m')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_raw_data[['Age', 'Height', 'Weight']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'operazione comporta la perdita di sole 25 istanze limite. Dai grafici e dalle statistiche generali delle feature si osserva che sono state rimossi i record con età superiore ai circa 44 anni, con un peso superiore ai circa 165kg; non sono stati cancellati record sulla base dei valori assunti dall'altezza.<br>\n",
    "Le colonne interessate hanno mantenuto la medesima media, andando invece a migliorare la relativa deviazione standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esplorazione delle feature continue di natura categorica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title('FCVC')\n",
    "fcvc = obesity_raw_data[\"FCVC\"].value_counts()\n",
    "plt.scatter(fcvc.keys(), fcvc.values, color=\"purple\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('frequenza')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title('NCP')\n",
    "ncp = obesity_raw_data[\"NCP\"].value_counts()\n",
    "plt.scatter(ncp.keys(), ncp.values)\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('n. pasti')\n",
    "\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title('CH2O')\n",
    "ch2o = obesity_raw_data[\"CH2O\"].value_counts()\n",
    "plt.scatter(ch2o.keys(), ch2o.values, color=\"tab:orange\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('l')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title('FAF')\n",
    "faf = obesity_raw_data[\"FAF\"].value_counts()\n",
    "plt.scatter(faf.keys(), faf.values, color=\"tab:red\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('giorni a settimana')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title('TUE')\n",
    "tue = obesity_raw_data[\"TUE\"].value_counts()\n",
    "plt.scatter(tue.keys(), tue.values, color=\"tab:green\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('ore giornaliere')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I grafici mostrati sono relativi alle sopracitate variabili di natura categorica ma trattate come continue. Si osserva che la maggior parte delle istanze ricadono precisamente in delle specifiche classi, mentre i restanti valori si distribuiscono in maniera uniforme tra le stesse.\n",
    "Vista la necessità di ricondursi a valori di tipo categorico si procede con l'arrotondamento al valore intero più vicino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_round_data = obesity_raw_data.copy()\n",
    "obesity_round_data['NCP'] = obesity_round_data['NCP'].apply(lambda x: 1 if 1 <= x < 2 else (3 if 2 <= x < 3.5 else round(x))).astype(int) \n",
    "obesity_round_data['FCVC'] = obesity_round_data['FCVC'].round().astype(int)\n",
    "obesity_round_data['CH2O'] = obesity_round_data['CH2O'].round().astype(int)\n",
    "obesity_round_data['FAF'] = obesity_round_data['FAF'].round().astype(int)\n",
    "obesity_round_data['TUE'] = obesity_round_data['TUE'].round().astype(int)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title('FCVC')\n",
    "fcvc = obesity_round_data[\"FCVC\"].value_counts()\n",
    "plt.scatter(fcvc.keys(), fcvc.values, color=\"purple\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('frequenza')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title('NCP')\n",
    "ncp = obesity_round_data[\"NCP\"].value_counts()\n",
    "plt.scatter(ncp.keys(), ncp.values)\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('n. pasti')\n",
    "\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title('CH2O')\n",
    "ch2o = obesity_round_data[\"CH2O\"].value_counts()\n",
    "plt.scatter(ch2o.keys(), ch2o.values, color=\"tab:orange\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('l')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title('FAF')\n",
    "faf = obesity_round_data[\"FAF\"].value_counts()\n",
    "plt.scatter(faf.keys(), faf.values, color=\"tab:red\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('giorni a settimana')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title('TUE')\n",
    "tue = obesity_round_data[\"TUE\"].value_counts()\n",
    "plt.scatter(tue.keys(), tue.values, color=\"tab:green\")\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('ore giornaliere')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sopra sono riportati i grafici delle feature dopo l'arrotondamento.\n",
    "\n",
    "Di seguito le statistiche generali delle variabili prima e dopo l'operazione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_raw_data[['NCP', 'FCVC', 'CH2O', 'FAF', 'TUE']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obesity_round_data[['NCP', 'FCVC', 'CH2O', 'FAF', 'TUE']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può notare, riportare i valori non interi all'interno delle classi contemplate ha avuto un impatto alquanto limitato sulle statistiche generali delle feature trattate. In particolare media e deviazione standard dimostrano come la distribuzione dei valori sia coerente con la precedene, grazie ad un corretto bilanciamento dei record generati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esplorazione delle feature categoriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color= ['tab:blue', 'tab:red', 'tab:green', 'tab:orange', 'tab:purple']\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Gender')\n",
    "plt.bar(obesity_raw_data['Gender'].unique(),\n",
    "        obesity_raw_data['Gender'].value_counts(),\n",
    "        color=color)\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('CAEC')\n",
    "plt.bar(obesity_raw_data['CAEC'].unique(),\n",
    "        obesity_raw_data['CAEC'].value_counts(),\n",
    "        color=color)\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('CALC')\n",
    "plt.bar(obesity_raw_data['CALC'].unique(),\n",
    "        obesity_raw_data['CALC'].value_counts(),\n",
    "        color=color)\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('MTRANS')\n",
    "plt.bar(obesity_raw_data['MTRANS'].unique(),\n",
    "        obesity_raw_data['MTRANS'].value_counts(),\n",
    "        color=color)\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dagli istrogrammi sopra osserviamo che i soggetti sono pressochè equamente distribuiti tra maschi e femmine, mangiano qualche volta tra i pasti, non bevono alcool o molto poco e si muovono principalmente con i trasporti pubblici o a piedi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esplorazione delle feature booleane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ['tab:blue', 'tab:red']\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('FHWO')\n",
    "plt.bar(obesity_raw_data['FHWO'].replace({True: \"SI\", False: \"NO\"}).unique(),\n",
    "        obesity_raw_data['FHWO'].value_counts(),\n",
    "        color=color)\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('FAVC')\n",
    "plt.bar(obesity_raw_data['FAVC'].replace({True: \"SI\", False: \"NO\"}).unique(),\n",
    "        obesity_raw_data['FAVC'].value_counts(),\n",
    "        color=color)\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('SMOKE')\n",
    "plt.bar(obesity_raw_data['SMOKE'].replace({True: \"SI\", False: \"NO\"}).unique(),\n",
    "        obesity_raw_data['SMOKE'].value_counts(),\n",
    "        color=color)\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('SCC')\n",
    "plt.bar(obesity_raw_data['SCC'].replace({True: \"SI\", False: \"NO\"}).unique(),\n",
    "        obesity_raw_data['SCC'].value_counts(),\n",
    "        color=color)\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I grafici mostrano un comportamento salubre della maggior parte del campione, essendo composto da pochi fumatori e pochi soggetti che mangiano regolarmente alimenti ad alto contenuto calorico (FAVC). La maggioranza non ha familiari che sono o sono stati sovrappeso e non monitora le calorie che assume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indici di correlazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione si valuteranno le possibili correlazioni tra le varie feature presenti nel dataset.\n",
    "\n",
    "La funzione sotto permette di rappresentare graficamente le varie matrici di correlazione attraverso una heatmap, permettendo una più facile analisi dei relativi coefficienti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation(dataset):\n",
    "    cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    sns.heatmap(dataset, cmap=cmap, vmax=.5, center=0, annot = True, square=True, linewidths=.5, annot_kws={\"size\": 12}, fmt=\".2f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si procede con la codifica delle classi delle variabili categoriche in valori numerici. Si distinguono le feature in continue, categoriche nominali e categoriche ordinali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = obesity_round_data.copy()\n",
    "\n",
    "dataset['CAEC'] = dataset['CAEC'].replace({\"no\": 0, \"Sometimes\": 1, \"Frequently\": 2, \"Always\": 3})\n",
    "dataset['CALC'] = dataset['CALC'].replace({\"no\": 0, \"Sometimes\": 1, \"Frequently\": 2, \"Always\": 3})\n",
    "dataset['Gender'] = dataset['Gender'].replace({\"Female\": 0, \"Male\": 1})\n",
    "dataset['MTRANS'] = dataset['MTRANS'].replace({\"Automobile\": 0, \"Motorbike\": 1, \"Bike\": 3, \"Public_Transportation\": 2, \"Walking\": 4})\n",
    "dataset['ObesityLevel'] = dataset['ObesityLevel'].replace({'Insufficient_Weight': 0, 'Normal_Weight': 1, 'Overweight_Level_I': 2, 'Overweight_Level_II': 3, 'Obesity_Type_I': 4, 'Obesity_Type_II': 5, 'Obesity_Type_III': 6})\n",
    "\n",
    "continuous_features = ['Age', 'Height', 'Weight']\n",
    "nominal_features = ['Gender', 'FHWO', 'FAVC', 'SMOKE', 'SCC', 'MTRANS']\n",
    "ordinal_features = ['FCVC', 'NCP', 'CAEC', 'CH2O', 'FAF', 'TUE', 'CALC', 'ObesityLevel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si calcolano gli indici di correlazione di Pearson tra le feature continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuos_correlation_matrix = obesity_round_data[continuous_features].corr()\n",
    "plot_correlation(continuos_correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nota che il peso è debolmente correlato all'età e moderatamente all'altezza.\n",
    "L'età risulta incorrelata all'altezza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si calcolano gli indici di correlazione V di Cramer tra le variabili nominali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_correlation_matrix = pd.DataFrame(index=nominal_features,\n",
    "                                     columns=nominal_features, dtype=float)\n",
    "for col1 in nominal_features:\n",
    "    for col2 in nominal_features:\n",
    "        if col1 != col2:\n",
    "            correlation = cramers_v(obesity_round_data[col1], obesity_round_data[col2])\n",
    "            nominal_correlation_matrix.loc[col1, col2] = correlation\n",
    "            nominal_correlation_matrix.loc[col2, col1] = correlation\n",
    "np.fill_diagonal(nominal_correlation_matrix.values, 1)\n",
    "\n",
    "plot_correlation(nominal_correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* il mezzo di trasporto preferito (`MTRANS`) ha una correlazione debole con il genere (`Gender`) e con il consumo di cibi ad alto contenuto calorico (`FAVC`).\n",
    "* il monitoraggio delle calorie assunte (`SCC`) ha una correlazione debole con il consumo di cibi ad alto contenuto calorico (`FAVC`) e con la presenza di familiari che sono o sono stati in sovrappeso (`FHWO`).\n",
    "* il consumo di cibi ad alto contenuto calorico (`FAVC`) ha una correlazione moderata con la presenza di familiari che sono o sono stati in sovrappeso (`FHWO`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si calcolano gli indici di correlazione di Kendall tra le feature categoriche ordinali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_correlation_matrix = dataset[ordinal_features].corr(method='kendall')\n",
    "plot_correlation(ordinal_correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si rileva che non ci sono particolari concordanze tra le variabili, ad eccezione di:\n",
    "* il numero di pasti consumati in una giornata (`NCP`) ha una concordanza molto debole con la frequenza di svolgimento di attività fisica (`FAF`).\n",
    "* il consumo di cibo tra i pasti (`CAEC`) ha una concordanza negativa molto debole con i litri d'acqua assunti (`CH20`).\n",
    "* il livello di obesità (`ObesityLevel`) ha una concordanza negativa molto debole con la frequenza di svolgimento di attività fisica (`FAF`), una concordanza negativa debole con il consumo di cibo tra i pasti (`CAEC`) e una concordanza molto debole con la frequenza di assunzione di alcool (`CALC`) e una concordanza molto debole con il consumo di verdure durante i pasti (`FCVC`) e con i litri di acqua assunti (`CH2O`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variabile categorica nominale `MTRANS` è multiclasse. Per questo è necessaria l'applicazione del one-hot encoding al fine produrre variabili binarie confrontabili con le altre feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=[\"MTRANS\"])\n",
    "nominal_features = ['Gender', 'FHWO', 'FAVC', 'SMOKE', 'SCC', 'MTRANS_0', 'MTRANS_1', 'MTRANS_2', 'MTRANS_3', 'MTRANS_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si calcolano gli indici di correlazione point-biserial tra le variabili categoriche nominali e le variabili continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "continuos_nominal_correlation_matrix = pd.DataFrame(index=continuous_features, columns=nominal_features, dtype=float)\n",
    "\n",
    "for col_cont in continuous_features:\n",
    "    for col_nom in nominal_features:\n",
    "        r, p_value = pointbiserialr(dataset[col_nom], dataset[col_cont])\n",
    "        continuos_nominal_correlation_matrix.loc[col_cont, col_nom] = r\n",
    "\n",
    "plot_correlation(continuos_nominal_correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* il peso (`Weight`) ha una correlazione debole con il consumo di cibo ad alto contenuto calorico (`FAVC`), una correlazione moderrata con la presenza di familiari che sono o sono stati in sovrappeso (`FHWO`) e una correlazione negativa debole con il monitoraggio delle calorie assunte(`SCC`).\n",
    "* l'altezza (`Height`) ha una correlazione debole con la presenza di familiari che sono o sono stati in sovrappeso (`FHWO`) e una correlazione moderata con il genere (`Gender`).\n",
    "* l'età (`Age`) ha una correlazione debole con la presenza di familiari che sono o sono stati in sovrappeso (`FHWO`), una correlazione moderata con l'utilizzo dell'auto come mezzo di trasporto (`MTRANS_0`) e una correlazione negativa moderata con l'utilizzo della bicicletta come mezzo di trasporto (`MTRANS_2`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si calcolano gli indici di correlazione di Spearman tra le variabili categoriche ordinali e quelle continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_continuous_correlation_matrix = dataset[ordinal_features + continuous_features].corr(method='spearman')\n",
    "plot_correlation(ordinal_continuous_correlation_matrix.iloc[-3:, :-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* l'età (`Age`) ha una correlazione negativa molto debole con la frequenza di svolgimento di attività fisica (`FAF`), una correlazione negativa debole con la frequenza di utilizzo di apparecchi tecnologici (`TUE`) e una correlazione moderata con il livello di obesità (`ObesityLevel`).\n",
    "* l'altezza (`Height`) ha una correlazione molto debole con i litri d'acqua assunti (`CH2O`), una correlazione debole con il numero di pasti consumati in una giornata (`NCP`) e con la frequenza di svolgimento di attività fisica (`FAF`).\n",
    "* il peso ha una correlazione negativa debole con il consumo di cibo tra i pasti (`CAEC`), una correlazione molto debole con i litri d'acqua assunti (`CH2O`) e con la frequenza di assunzione di alcool (`CALC`) e una correlazione molto forte con il livello di obesità (`ObesityLevel`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si calcolano gli indici di correlazione point-biserial tra le variabili categoriche nominali e quelle ordinali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_nominal_correlation_matrix = pd.DataFrame(index=ordinal_features, columns=nominal_features, dtype=float)\n",
    "\n",
    "for col_ord in ordinal_features:\n",
    "    for col_nom in nominal_features:\n",
    "        r, p_value = pointbiserialr(dataset[col_nom], dataset[col_ord])\n",
    "        ordinal_nominal_correlation_matrix.loc[col_ord, col_nom] = r\n",
    "\n",
    "plot_correlation(ordinal_nominal_correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* il consumo di verdura durante i pasti (`FCVC`) ha una correlazione negativa molto debole con il genere (`Gender`). \n",
    "* la frequenza di svolgimento di attività fisica (`FAF`) ha una correlazione molto debole con il genere (`Gender`).\n",
    "* il consumo di cibo tra i pasti (`CAEC`) ha una correlazione negativa molto debolo con la presenza di familiari che sono o sono stati sovrappeso (`FHWO`) e con il consumo di cibi ad alto contenuto calorico (`FAVC`).\n",
    "* i litri di acqua assunti in una giornata (`CH2O`) ha una correlazione molto debole con la presenza di familiari che sono o sono stati sovrappeso (`FHWO`).\n",
    "* la frequenza di utilizzo di dispositivi elettronici (`TUE`) ha una correlazione negativa molto debole con l'utilizzo dell'automobile come mezzo di trasporto (`MTRANS_0`) e una correlazione molto debole con l'utilizzo della bicicletta (`MTRANS_2`).\n",
    "* il livello di obesità (`ObesityLevel`) ha una correlazione negativa molto debole con il monitoraggio delle calorie giornaliere (`SCC`), una correlazione debole con il consumo di cibo ad alto contenuto calorico (`FAVC`) e una correlazione moderata con la presenza di familiari che sono o sono stati sovrappeso (`FHWO`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3 - Preparazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si calcola l'indice di massa corporea (BMI) di ogni soggetto con la seguente formula\n",
    "\\begin{equation}\n",
    "BMI = \\frac{weight_{kg}}{height_{m}^2}\n",
    "\\end{equation}\n",
    "e si aggiunge la nuova feature ottenuta al dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['BMI'] = dataset['Weight'] / (dataset['Height'])**2\n",
    "\n",
    "ordinal_features = ['FCVC', 'NCP', 'CAEC', 'CH2O', 'FAF', 'TUE', 'CALC', 'ObesityLevel', 'BMI']\n",
    "\n",
    "ordinal_correlation_matrix = dataset[ordinal_features].corr(method='kendall')\n",
    "plot_correlation(ordinal_correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si osserva la correlazione molto forte della nuova feature (`BMI`) con la variabile target (`ObesityLevel`). Si suppone quindi che sia rilevante per la modellazione; in seguito verrà ne verrà valutato l'impatto effettivo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si procede con l'isolamento della variabile target (`ObesityLevel`) e la suddivisione del dataset in training set e validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.drop('ObesityLevel', axis=1)\n",
    "y = dataset['ObesityLevel']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=43, stratify=y.values)\n",
    "\n",
    "X_no_BMI = dataset.drop(['ObesityLevel', 'BMI'] , axis=1)\n",
    "y_no_BMI = dataset['ObesityLevel']\n",
    "X_train_no_BMI, X_val_no_BMI, y_train_no_BMI, y_val_no_BMI = train_test_split(X_no_BMI, y_no_BMI, test_size=0.3, random_state=43, stratify=y.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito viene allenato un modello lineare per verificare l'effettivo impatto dell'aggiunta della feature BMI.<br>\n",
    "Si valuta inoltre l'utilità di eseguire one-hot encoding sulle feature nominali del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ordinal_features = ['FCVC', 'NCP', 'CAEC', 'CH2O', 'FAF', 'TUE', 'CALC', 'BMI']\n",
    "nominal_features = [\"Gender\", \"FHWO\", \"FAVC\", \"SMOKE\", \"SCC\"]\n",
    "\n",
    "std_perceptron = Pipeline([\n",
    "    (\"preproc\", ColumnTransformer([\n",
    "        (\"numeric\", ..., continuous_features),\n",
    "        (\"nominal\", ..., nominal_features),\n",
    "        (\"ordinal\", ..., ordinal_features),\n",
    "    ])),\n",
    "    ('perceptron', OneVsRestClassifier(Perceptron(n_jobs=-1, early_stopping=True, n_iter_no_change=5)))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'preproc__numeric': ['passthrough', StandardScaler()],\n",
    "    'preproc__nominal': ['passthrough', OneHotEncoder()],\n",
    "    'preproc__ordinal': ['passthrough', StandardScaler()],\n",
    "    'perceptron__estimator__penalty': [None, 'l1', 'l2', 'elasticnet'],\n",
    "    'perceptron__estimator__alpha': [0.0001, 0.001, 0.01, 1],\n",
    "    'perceptron__estimator__tol': [1e-9, 1e-6, 1e-3, 1, 1e3, 1e6],\n",
    "}\n",
    "\n",
    "perceptron_cv = GridSearchCV(std_perceptron, parameters, cv=5, n_jobs=-1, scoring='f1_micro', error_score=\"raise\")\n",
    "perceptron_cv.fit(X_train, y_train)\n",
    "print('GridSearch on Perceptron finish')\n",
    "\n",
    "predictions = perceptron_cv.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print('Best parameters:', perceptron_cv.best_params_)\n",
    "print('Best score: {:.4f}%'.format(round(perceptron_cv.best_score_ * 100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train {:.2f}%'.format(perceptron_cv.score(X_train, y_train)*100))\n",
    "print('Accuracy on val {:.2f}%'.format(perceptron_cv.score(X_val, y_val)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['FCVC', 'NCP', 'CAEC', 'CH2O', 'FAF', 'TUE', 'CALC']\n",
    "\n",
    "std_perceptron = Pipeline([\n",
    "    (\"preproc\", ColumnTransformer([\n",
    "        (\"numeric\", ..., continuous_features),\n",
    "        (\"nominal\", ..., nominal_features),\n",
    "        (\"ordinal\", ..., ordinal_features),\n",
    "    ])),\n",
    "    ('perceptron', OneVsRestClassifier(Perceptron(n_jobs=-1, early_stopping=True, n_iter_no_change=5)))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'preproc__numeric': ['passthrough', StandardScaler()],\n",
    "    'preproc__nominal': ['passthrough', OneHotEncoder()],\n",
    "    'preproc__ordinal': ['passthrough', StandardScaler()],\n",
    "    'perceptron__estimator__penalty': [None, 'l1', 'l2', 'elasticnet'],\n",
    "    'perceptron__estimator__alpha': [0.0001, 0.001, 0.01, 1],\n",
    "    'perceptron__estimator__tol': [1e-9, 1e-6, 1e-3, 1, 1e3, 1e6],\n",
    "}\n",
    "\n",
    "perceptron_cv_no_BMI = GridSearchCV(std_perceptron, parameters, cv=5, n_jobs=-1, scoring='f1_micro', error_score=\"raise\")\n",
    "perceptron_cv_no_BMI.fit(X_train_no_BMI, y_train_no_BMI)\n",
    "print('GridSearch on Perceptron without BMI finish')\n",
    "\n",
    "predictions = perceptron_cv_no_BMI.predict(X_val_no_BMI)\n",
    "\n",
    "accuracy = accuracy_score(y_val_no_BMI, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print('Best parameters:', perceptron_cv_no_BMI.best_params_)\n",
    "print('Best score: {:.4f}%'.format(round(perceptron_cv_no_BMI.best_score_ * 100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train {:.2f}%'.format(perceptron_cv_no_BMI.score(X_train_no_BMI, y_train_no_BMI)*100))\n",
    "print('Accuracy on val {:.2f}%'.format(perceptron_cv_no_BMI.score(X_val_no_BMI, y_val_no_BMI)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alla luce dei risultati delle grid search si conclude che la presenza della feature BMI comporta un leggero aumento dell'accuratezza. L'applicazione del one-hot encoding alle variabili nominali, seppur binarie, viene in entrambi i casi indicata come la scelta migliore.\n",
    "\n",
    "Si procede di seguito con l'applicazione della codifica sul dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_features = [\"Gender_0\",\"Gender_1\", \"FHWO_False\", \"FHWO_True\", \"FAVC_False\", \"FAVC_True\", \"SMOKE_False\", \"SMOKE_True\", \"SCC_False\", \"SCC_True\", 'MTRANS_0', 'MTRANS_1', 'MTRANS_2', 'MTRANS_3', 'MTRANS_4']\n",
    "ordinal_features = ['FCVC', 'NCP', 'CAEC', 'CH2O', 'FAF', 'TUE', 'CALC', 'BMI']\n",
    "\n",
    "dataset = pd.get_dummies(dataset, columns=[\"Gender\", \"FHWO\", \"FAVC\", \"SMOKE\", \"SCC\"])\n",
    "\n",
    "X = dataset.drop('ObesityLevel', axis=1)\n",
    "y = dataset['ObesityLevel']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=43, stratify=y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4 - Modellazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "def confidence_interval(instance, acc, confidence):    \n",
    "    return proportion_confint(instance * acc, instance, 1-confidence/100, method='wilson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito si riporta l'addestramento di alcuni modelli per la classificazione multi-classe e si esegue la Grid Search per ricavare gli iperparametri migliori.\n",
    "Per ogni modello vengono mostrate l'accuratezza, la matrice di confusione e le feature più rilevanti. Dal momento che la variabile target è multi-classe, durante l'allenamento otteniamo pesi delle feature distinti per ciascun iperpiano. Per ottenere una visione complessiva dell'importanza delle features, combiniamo questi pesi calcolandone la media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_perceptron = Pipeline([\n",
    "    (\"preproc\", ColumnTransformer([\n",
    "        (\"numeric\", StandardScaler(), continuous_features),\n",
    "        (\"nominal\", \"passthrough\", nominal_features),\n",
    "        (\"ordinal\", StandardScaler(), ordinal_features)\n",
    "    ])),\n",
    "    ('perceptron', OneVsRestClassifier(Perceptron(n_jobs=-1, early_stopping=True, n_iter_no_change=5)))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'perceptron__estimator__penalty': [None, 'l1', 'l2', 'elasticnet'],\n",
    "    'perceptron__estimator__alpha': [0.0001, 0.001, 0.01, 1],\n",
    "    'perceptron__estimator__tol': [1e-9, 1e-6, 1e-3, 1, 1e3, 1e6],\n",
    "}\n",
    "\n",
    "perceptron_cv = GridSearchCV(std_perceptron, parameters, cv=5, n_jobs=-1, scoring='f1_micro', error_score=\"raise\")\n",
    "perceptron_cv.fit(X_train, y_train)\n",
    "print('GridSearch on Perceptron finished')\n",
    "\n",
    "predictions = perceptron_cv.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print('Best parameters:', perceptron_cv.best_params_)\n",
    "print('Best score: {:.4f}%'.format(round(perceptron_cv.best_score_ * 100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train {:.2f}%'.format(perceptron_cv.score(X_train, y_train)*100))\n",
    "print('Accuracy on val {:.2f}%'.format(perceptron_cv.score(X_val, y_val)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 95\n",
    "lower, upper = confidence_interval(len(X_val), perceptron_cv.score(X_val, y_val), confidence)\n",
    "\n",
    "print('Interval with confidence {}%: \\nPmin = {:.4f}%\\nPmax = {:.4f}%'.format(confidence, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = perceptron_cv.best_estimator_.named_steps['perceptron'].estimators_\n",
    "weights_per_class = [classifier.coef_ for classifier in classifiers]\n",
    "avg_importance = np.mean(np.abs(weights_per_class), axis=0)[0]\n",
    "feature_importance = pd.DataFrame({'Feature': X_val.columns, 'Importance': avg_importance})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
    "\n",
    "feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si può notare che le feature più rilevanti rispettano le aspettative in quanto principalmente legate alle abitudini alimentari dei soggetti. Evidenziamo inoltre l'inaspettata importanza della feature `SCC_True` che indica che se il soggetto monitora le calorie che assume durante il giorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = perceptron_cv.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plot_confusion_matrix(cm, target_names=['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = perceptron_cv.predict(X_val)\n",
    "print(classification_report(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "perceptron_mse = mean_squared_error(y_val, perceptron_cv.predict(X_val))\n",
    "print('MSE: {}'.format(perceptron_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalle analisi riportate sopra si evince che il modello non è particolarmente accurato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    (\"preproc\", ColumnTransformer([\n",
    "        (\"numeric\", StandardScaler(), continuous_features),\n",
    "        (\"nominal\", \"passthrough\", nominal_features),\n",
    "        (\"ordinal\", StandardScaler(), ordinal_features)\n",
    "    ])),\n",
    "    ('svm', SVC(decision_function_shape='ovr'))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svm__degree': [2, 3, 4],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "}\n",
    "\n",
    "svm_cv = GridSearchCV(svm_pipeline, parameters, cv=5, n_jobs=-1, scoring='f1_micro', error_score='raise')\n",
    "svm_cv.fit(X_train, y_train)\n",
    "print('GridSearch on SVM finished')\n",
    "\n",
    "predictions = svm_cv.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print('Best parameters:', svm_cv.best_params_)\n",
    "print('Best score: {:.4f}%'.format(round(svm_cv.best_score_ * 100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train {:.2f}%'.format(svm_cv.score(X_train, y_train)*100))\n",
    "print('Accuracy on val {:.2f}%'.format(svm_cv.score(X_val, y_val)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 95\n",
    "lower, upper = confidence_interval(len(X_val), svm_cv.score(X_val, y_val), confidence)\n",
    "\n",
    "print('Interval with confidence {}%: \\nPmin = {:.4f}%\\nPmax = {:.4f}%'.format(confidence, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = svm_cv.best_estimator_.named_steps['svm'].coeff_\n",
    "weights_per_class = [classifier for classifier in classifiers]\n",
    "avg_importance = np.mean(np.abs(weights_per_class), axis=0)\n",
    "feature_importance = pd.DataFrame({'Feature': X_val.columns, 'Importance': avg_importance})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
    "\n",
    "feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche questo modello conferma `SCC_True` come la feature più rilevante, seguita da `Weight` e `Height`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_cv.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plot_confusion_matrix(cm, target_names=['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svm_cv.predict(X_val)\n",
    "print(classification_report(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_mse = mean_squared_error(y_val, svm_cv.predict(X_val))\n",
    "print('MSE: {}'.format(svm_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "linear_log_reg_pipeline = Pipeline([\n",
    "    (\"preproc\", ColumnTransformer([\n",
    "        (\"numeric\", StandardScaler(), continuous_features),\n",
    "        (\"nominal\", \"passthrough\", nominal_features),\n",
    "        (\"ordinal\", StandardScaler(), ordinal_features)\n",
    "    ])),\n",
    "    ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=10000))\n",
    "])\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'log_reg__penalty': ['l1', 'l2'],\n",
    "        'log_reg__C': [0.1, 1, 10],\n",
    "    },\n",
    "    {\n",
    "        'log_reg__penalty': ['elasticnet'],\n",
    "        'log_reg__C': [1],\n",
    "        \"log_reg__l1_ratio\": [0.5]\n",
    "    }\n",
    "]\n",
    "\n",
    "linear_log_reg_cv = GridSearchCV(linear_log_reg_pipeline, parameters, cv=5, n_jobs=-1, scoring='f1_micro', error_score='raise')\n",
    "linear_log_reg_cv.fit(X_train, y_train)\n",
    "print('GridSearch on Polynomial Logistic Regression finished')\n",
    "\n",
    "predictions = linear_log_reg_cv.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print('Best parameters:', linear_log_reg_cv.best_params_)\n",
    "print('Best score: {:.4f}%'.format(round(linear_log_reg_cv.best_score_ * 100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train {:.2f}%'.format(linear_log_reg_cv.score(X_train, y_train)*100))\n",
    "print('Accuracy on val {:.2f}%'.format(linear_log_reg_cv.score(X_val, y_val)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 95\n",
    "lower, upper = confidence_interval(len(X_val), linear_log_reg_cv.score(X_val, y_val), confidence)\n",
    "\n",
    "print('Interval with confidence {}%: \\nPmin = {:.4f}%\\nPmax = {:.4f}%'.format(confidence, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = linear_log_reg_cv.best_estimator_.named_steps['log_reg'].coef_\n",
    "weights_per_class = [classifier for classifier in classifiers]\n",
    "avg_importance = np.mean(np.abs(weights_per_class), axis=0)\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': X_val.columns, 'Importance': avg_importance})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
    "\n",
    "feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_log_reg_cv.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plot_confusion_matrix(cm, target_names=['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = linear_log_reg_cv.predict(X_val)\n",
    "print(classification_report(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_log_reg_mse = mean_squared_error(y_val, linear_log_reg_cv.predict(X_val))\n",
    "print('MSE: {}'.format(linear_log_reg_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_log_reg_pipeline = Pipeline([\n",
    "    (\"preproc\", ColumnTransformer([\n",
    "        (\"numeric\", StandardScaler(), continuous_features),\n",
    "        (\"nominal\", \"passthrough\", nominal_features),\n",
    "        (\"ordinal\", StandardScaler(), ordinal_features)\n",
    "    ])),\n",
    "    (\"poly\",   PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=10000))\n",
    "])\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'log_reg__penalty': ['l1', 'l2'],\n",
    "        'log_reg__C': [0.1, 1, 10],\n",
    "    },\n",
    "    {\n",
    "        'log_reg__penalty': ['elasticnet'],\n",
    "        'log_reg__C': [1],\n",
    "        \"log_reg__l1_ratio\": [0.5]\n",
    "    }\n",
    "]\n",
    "\n",
    "poly_log_reg_cv = GridSearchCV(poly_log_reg_pipeline, parameters, cv=5, n_jobs=-1, scoring='f1_micro', error_score='raise')\n",
    "poly_log_reg_cv.fit(X_train, y_train)\n",
    "print('GridSearch on Polynomial Logistic Regression finished')\n",
    "\n",
    "predictions = poly_log_reg_cv.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print('Best parameters:', poly_log_reg_cv.best_params_)\n",
    "print('Best score: {:.4f}%'.format(round(poly_log_reg_cv.best_score_ * 100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train {:.2f}%'.format(poly_log_reg_cv.score(X_train, y_train)*100))\n",
    "print('Accuracy on val {:.2f}%'.format(poly_log_reg_cv.score(X_val, y_val)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 95\n",
    "lower, upper = confidence_interval(len(X_val), poly_log_reg_cv.score(X_val, y_val), confidence)\n",
    "\n",
    "print('Interval with confidence {}%: \\nPmin = {:.4f}%\\nPmax = {:.4f}%'.format(confidence, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = poly_log_reg_cv.best_estimator_.named_steps['log_reg'].coef_\n",
    "weights_per_class = [classifier for classifier in classifiers]\n",
    "avg_importance = np.mean(np.abs(weights_per_class), axis=0)\n",
    "feature_importance = pd.DataFrame({'Feature': poly_log_reg_cv.best_estimator_.named_steps['poly'].get_feature_names_out(X_val.columns), 'Importance': avg_importance})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
    "\n",
    "feature_importance.tail(26).plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = poly_log_reg_cv.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plot_confusion_matrix(cm, target_names=['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = poly_log_reg_cv.predict(X_val)\n",
    "print(classification_report(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_log_reg_mse = mean_squared_error(y_val, poly_log_reg_cv.predict(X_val))\n",
    "print('MSE: {}'.format(poly_log_reg_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_pipeline = Pipeline([\n",
    "    (\"preproc\", ColumnTransformer([\n",
    "        (\"numeric\", StandardScaler(), continuous_features),\n",
    "        (\"nominal\", \"passthrough\", nominal_features),\n",
    "        (\"ordinal\", StandardScaler(), ordinal_features)\n",
    "    ])),\n",
    "    ('rfc', RandomForestClassifier(n_jobs=-1, random_state=3))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'rfc__n_estimators': [100, 200, 300],\n",
    "    'rfc__max_depth': [2, 4, 6, 8, 10],\n",
    "    'rfc__min_samples_leaf': [1, 2, 4],\n",
    "    'rfc__min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "rfc_cv = GridSearchCV(rfc_pipeline, parameters, cv=5, n_jobs=-1, scoring='f1_micro', error_score='raise')\n",
    "rfc_cv.fit(X_train, y_train)\n",
    "print('GridSearch on Random Forest finished')\n",
    "\n",
    "predictions = rfc_cv.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print('Best parameters:', rfc_cv.best_params_)\n",
    "print('Best score: {:.4f}%'.format(round(rfc_cv.best_score_ * 100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train {:.2f}%'.format(rfc_cv.score(X_train, y_train)*100))\n",
    "print('Accuracy on val {:.2f}%'.format(rfc_cv.score(X_val, y_val)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 95\n",
    "lower, upper = confidence_interval(len(X_val), rfc_cv.score(X_val, y_val), confidence)\n",
    "\n",
    "print('Interval with confidence {}%: \\nPmin = {:.4f}%\\nPmax = {:.4f}%'.format(confidence, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'Feature': X_val.columns, 'Importance': rfc_cv.best_estimator_.named_steps['rfc'].feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
    "\n",
    "feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc_cv.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plot_confusion_matrix(cm, target_names=['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rfc_cv.predict(X_val)\n",
    "print(classification_report(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_mse = mean_squared_error(y_val, rfc_cv.predict(X_val))\n",
    "print('MSE: {}'.format(rfc_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "rfc_pipeline = Pipeline([\n",
    "    (\"preproc\", ColumnTransformer([\n",
    "        (\"numeric\", StandardScaler(), continuous_features),\n",
    "        (\"nominal\", \"passthrough\", nominal_features),\n",
    "        (\"ordinal\", StandardScaler(), ordinal_features)\n",
    "    ])),\n",
    "    ('xgb', XGBClassifier(nthread=8, objective='multi:softprob'))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'xgb__eta': [0.001, 0.1, 0.5],\n",
    "    'xgb__max_depth': [3, 5, 7],\n",
    "    'xgb__n_estimators': [150, 300],\n",
    "    'xgb__alpha': [0.0001, 0.001],\n",
    "}\n",
    "\n",
    "rfc_cv = GridSearchCV(rfc_pipeline, parameters, cv=5, n_jobs=-1, scoring='f1_micro', error_score='raise')\n",
    "rfc_cv.fit(X_train, y_train)\n",
    "print('GridSearch on Random Forest finished')\n",
    "\n",
    "predictions = rfc_cv.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print('Best parameters:', rfc_cv.best_params_)\n",
    "print('Best score: {:.4f}%'.format(round(rfc_cv.best_score_ * 100, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train {:.2f}%'.format(xgb_cv.score(X_train, y_train)*100))\n",
    "print('Accuracy on val {:.2f}%'.format(xgb_cv.score(X_val, y_val)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 95\n",
    "lower, upper = confidence_interval(len(X_val), xgb_cv.score(X_val, y_val), confidence)\n",
    "\n",
    "print('Interval with confidence {}%: \\nPmin = {:.4f}%\\nPmax = {:.4f}%'.format(confidence, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'Feature': X_val.columns, 'Importance': xgb_cv.best_estimator_.named_steps['xgb'].feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
    "\n",
    "feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_cv.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plot_confusion_matrix(cm, target_names=['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_cv.predict(X_val)\n",
    "print(classification_report(y_val, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_mse = mean_squared_error(y_val, xgb_cv.predict(X_val))\n",
    "print('MSE: {}'.format(xgb_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confronto tra i modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def confidence_interval_between_models(model1, model2, confidence):\n",
    "    z_half_alfa = stats.norm.ppf(confidence)\n",
    "    y1_pred = model1.predict(X_val)\n",
    "    y2_pred = model2.predict(X_val)\n",
    "    error1 = 1 - f1_score(y_val, y1_pred, average=\"micro\")\n",
    "    error2 = 1 - f1_score(y_val, y2_pred, average=\"micro\")\n",
    "\n",
    "    variance = (((1 - error1) * error1) / len(y_val)) + (((1 - error2) * error2) / len(y_val))\n",
    "    d_minus = abs(error1 - error2) - z_half_alfa * (pow(variance, 0.5))\n",
    "    d_plus = abs(error1 - error2) + z_half_alfa * (pow(variance, 0.5))\n",
    "    print(\"Valore minimo: {}\\nValore massimo: {}\\n\".format(d_minus, d_plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"XGBoost vs Perceptron, intervallo di confidenza:\")\n",
    "confidence_interval_between_models(xgb_cv, perceptron_cv, 0.95)\n",
    "\n",
    "print(\"XGBoost vs SVM, intervallo di confidenza:\")\n",
    "confidence_interval_between_models(xgb_cv, svm_cv, 0.95)\n",
    "\n",
    "print(\"XGBoost vs Linear Logistic Regression, intervallo di confidenza:\")\n",
    "confidence_interval_between_models(xgb_cv, linear_log_reg_cv, 0.95)\n",
    "\n",
    "print(\"XGBoost vs Polynomial Logistic Regression, intervallo di confidenza:\")\n",
    "confidence_interval_between_models(xgb_cv, poly_log_reg_cv, 0.95)\n",
    "\n",
    "print(\"XGBoost vs Random Forest, intervallo di confidenza:\")\n",
    "confidence_interval_between_models(xgb_cv, rfc_cv, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest vs Percetron, intervallo di confidenza:\")\n",
    "confidence_interval_between_models(rfc_cv, perceptron_cv, 0.95)\n",
    "\n",
    "print(\"Random Forest vs Linear Logistic Regression, intervallo di confidenza:\")\n",
    "confidence_interval_between_models(rfc_cv, linear_log_reg_cv, 0.95)\n",
    "\n",
    "print(\"Random Forest vs Polynomial Logistic Regression, intervallo di confidenza:\")\n",
    "confidence_interval_between_models(rfc_cv, poly_log_reg_cv, 0.95)\n",
    "\n",
    "print(\"Random Forest vs SVM, intervallo di confidenza:\")\n",
    "confidence_interval_between_models(rfc_cv, svm_cv, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confrontando i modelli si nota che la differenza tra XGBoost e Random Forest non è statisticamente rilevante in quanto l'intervallo di confidenza contiene lo 0. Lo stesso vale per i modelli Linear Logistic Regression e SVM.\n",
    "\n",
    "Inoltre si osserva che la Linear Logistic Regression ottiene risultati migliori rispetto alla Polynomial Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
